"""
End-to-End Data Processing Pipeline
===================================

This module is the command-line entry point for data processing, chaining parsers → formatters → file saving.

Supported stages:
    parse        — Parse raw data → Unified Intermediate Format JSONL (data/preprocessed/)
    synthesize   — Synthesize Type B data (Requires Gemini API)
    format_sft   — Unified Intermediate Format → SFT Training Data (data/sft/)
    format_grpo  — SemEval Data → GRPO prompt data (data/grpo/)
    format_reward — Unified Intermediate Format → Reward Model Preference Pair Data (data/reward/)
    all          — Execute all above stages sequentially (excluding synthesis)
    full         — Execute all stages sequentially, including Type B synthesis (Requires Gemini API)

Usage:
    python -m data_preprocessing.pipeline --stage parse
    python -m data_preprocessing.pipeline --stage synthesize
    python -m data_preprocessing.pipeline --stage format_sft
    python -m data_preprocessing.pipeline --stage format_grpo
    python -m data_preprocessing.pipeline --stage format_reward
    python -m data_preprocessing.pipeline --stage all
    python -m data_preprocessing.pipeline --stage full

Each stage can run independently. format_sft / format_grpo / format_reward depend on the output of parse.
If parse has already run, subsequent stages will load intermediate results from data/preprocessed/
without re-parsing raw data.

Directory Structure Convention:
    Project Root/
    ├── data/
    │   ├── raw/              # Raw data (Input)
    │   ├── preprocessed/     # Unified Intermediate Format (parse output)
    │   │   ├── unified_all.jsonl   # All humor data (Single file)
    │   │   └── semeval.jsonl       # SemEval data (Different schema, stored separately)
    │   ├── synthesized/      # Type B synthesized data (Generated by independent script)
    │   ├── sft/              # SFT training data (format_sft output)
    │   ├── grpo/             # GRPO prompt data (format_grpo output)
    │   └── reward/           # Reward preference pairs (format_reward output)

Dependencies:
    - parsers.py
    - formatters.py
    - datasets (HuggingFace)
    - argparse (Standard library)
"""

import argparse
from pathlib import Path

import datasets

from data_preprocessing.parsers import parse_all
from data_preprocessing.formatters import (
    format_sft,
    format_grpo,
    format_reward_pairs,
)
from data_preprocessing.synthesize_task_data import (
    synthesize_for_language,
    save_synthesized,
)


# ============================================================
# Path Constants
# ============================================================
# Relative paths based on project root
# pipeline.py is in data_preprocessing/, project root is its parent's parent

PROJECT_ROOT = Path(__file__).resolve().parent.parent
DATA_DIR = PROJECT_ROOT / "data"

RAW_DIR = DATA_DIR / "raw"
PREPROCESSED_DIR = DATA_DIR / "preprocessed"
SYNTHESIZED_DIR = DATA_DIR / "synthesized"
SFT_DIR = DATA_DIR / "sft"
GRPO_DIR = DATA_DIR / "grpo"
REWARD_DIR = DATA_DIR / "reward"

# Intermediate format filenames
UNIFIED_ALL_FILE = PREPROCESSED_DIR / "unified_all.jsonl"
SEMEVAL_FILE = PREPROCESSED_DIR / "semeval.jsonl"


# ============================================================
# Stage 1: parse — Parse Raw Data → Unified Intermediate Format
# ============================================================

def run_parse() -> None:
    """Parse all raw data sources, save as JSONL files.

    Input: Raw datasets under data/raw/
    Output:
        - data/preprocessed/unified_all.jsonl  (All humor data, Unified Intermediate Format)
        - data/preprocessed/semeval.jsonl      (SemEval data, GRPO dedicated format)
    """
    PREPROCESSED_DIR.mkdir(parents=True, exist_ok=True)

    # 1. Parse all raw data
    all_data = parse_all(RAW_DIR)

    # 2. Merge unified intermediate format sources into one Dataset
    #    Exclude semeval (its schema is different)
    unified_source_names = [k for k in all_data if k != "semeval"]
    unified_parts = [all_data[name] for name in unified_source_names]
    unified_all = datasets.concatenate_datasets(unified_parts)

    # 3. Save Unified Intermediate Format (Single file)
    unified_all.to_json(str(UNIFIED_ALL_FILE))
    print(f"  Saved: {UNIFIED_ALL_FILE} ({len(unified_all)} rows)")

    # 4. Save SemEval data (Separate file, different schema)
    all_data["semeval"].to_json(str(SEMEVAL_FILE))
    print(f"  Saved: {SEMEVAL_FILE} ({len(all_data['semeval'])} rows)")


# ============================================================
# Stage 2: synthesize — Synthesize Type B Data
# ============================================================

def run_synthesize(
    n_headline: int = 200,
    n_keyword: int = 100,
    seed: int = 42,
) -> None:
    """Synthesize Type B task formatted data (Requires Gemini API).

    Synthesize headline and keyword subtask data for en/zh/es languages respectively.

    Input: Babel Briefings (Auto download) + Built-in keyword vocabulary
    Output: data/synthesized/type_b_{en,zh,es}.jsonl

    Args:
        n_headline: Number of headline subtask samples to synthesize per language
        n_keyword: Number of keyword subtask samples to synthesize per language
        seed: Random seed
    """
    for lang in ["en", "zh", "es"]:
        print(f"\n  Synthesizing {lang} data (headline: {n_headline}, keyword: {n_keyword})...")
        samples = synthesize_for_language(
            lang=lang,
            n_headline=n_headline,
            n_keyword=n_keyword,
            seed=seed,
        )
        output_path = save_synthesized(samples, lang)
        print(f"  Saved: {output_path} ({len(samples)} rows)")


# ============================================================
# Stage 3: format_sft — Unified Intermediate Format → SFT Training Data
# ============================================================

def run_format_sft() -> None:
    """Generate SFT training data from intermediate format.

    Input:
        - data/preprocessed/unified_all.jsonl (Output of parse stage)
        - data/synthesized/type_b_*.jsonl (Synthesized Type B data, optional)
    Output:
        - data/sft/sft_train.jsonl
        - data/sft/sft_val.jsonl
    """
    SFT_DIR.mkdir(parents=True, exist_ok=True)

    # 1. Load Unified Intermediate Format, grouped by source
    unified_datasets = _load_unified_datasets()

    # 2. Call formatter
    sft_ds = format_sft(
        unified_datasets,
        synthesized_dir=SYNTHESIZED_DIR,
    )

    # 3. Save
    sft_ds["train"].to_json(str(SFT_DIR / "sft_train.jsonl"))
    sft_ds["validation"].to_json(str(SFT_DIR / "sft_val.jsonl"))
    print(f"  Saved: {SFT_DIR / 'sft_train.jsonl'} ({len(sft_ds['train'])} rows)")
    print(f"  Saved: {SFT_DIR / 'sft_val.jsonl'} ({len(sft_ds['validation'])} rows)")


# ============================================================
# Stage 3: format_grpo — SemEval Data → GRPO Prompt
# ============================================================

def run_format_grpo() -> None:
    """Generate GRPO prompt from SemEval data.

    Input: data/preprocessed/semeval.jsonl (Output of parse stage)
    Output: data/grpo/grpo_prompts.jsonl
    """
    GRPO_DIR.mkdir(parents=True, exist_ok=True)

    # 1. Load SemEval intermediate data
    if not SEMEVAL_FILE.exists():
        raise FileNotFoundError(
            f"{SEMEVAL_FILE} does not exist, please run --stage parse first"
        )
    semeval_ds = datasets.load_dataset(
        "json", data_files=str(SEMEVAL_FILE), split="train"
    )
    print(f"  Loaded: {SEMEVAL_FILE} ({len(semeval_ds)} rows)")

    # 2. Call formatter
    grpo_ds = format_grpo(semeval_ds)

    # 3. Save
    output_path = GRPO_DIR / "grpo_prompts.jsonl"
    grpo_ds.to_json(str(output_path))
    print(f"  Saved: {output_path} ({len(grpo_ds)} rows)")


# ============================================================
# Stage 4: format_reward — Unified Intermediate Format → Preference Pairs
# ============================================================

def run_format_reward() -> None:
    """Construct Reward Model preference pairs data from intermediate format.

    Input: data/preprocessed/unified_all.jsonl (Output of parse stage)
    Output:
        - data/reward/preference_train.jsonl
        - data/reward/preference_val.jsonl
    """
    REWARD_DIR.mkdir(parents=True, exist_ok=True)

    # 1. Load Unified Intermediate Format, grouped by source
    unified_datasets = _load_unified_datasets()

    # 2. Call formatter
    reward_ds = format_reward_pairs(unified_datasets)

    # 3. Save
    reward_ds["train"].to_json(str(REWARD_DIR / "preference_train.jsonl"))
    reward_ds["validation"].to_json(str(REWARD_DIR / "preference_val.jsonl"))
    print(f"  Saved: {REWARD_DIR / 'preference_train.jsonl'} ({len(reward_ds['train'])} rows)")
    print(f"  Saved: {REWARD_DIR / 'preference_val.jsonl'} ({len(reward_ds['validation'])} rows)")


# ============================================================
# Helper Functions
# ============================================================

def _load_unified_datasets() -> dict[str, datasets.Dataset]:
    """Load Unified Intermediate Format data from data/preprocessed/unified_all.jsonl,
    group by source field, restore as dict[str, Dataset].

    This ensures formatters receive the same data structure as directly calling parse_all().

    Returns:
        dict: Dataset dictionary organized by source name.
            Keys are "rjokes", "cfun", "haha", "chinese_humor",
            Values are Datasets, schema: {text, lang, score, source}.

    Raises:
        FileNotFoundError: unified_all.jsonl does not exist
    """
    if not UNIFIED_ALL_FILE.exists():
        raise FileNotFoundError(
            f"{UNIFIED_ALL_FILE} does not exist, please run --stage parse first"
        )

    # Load single file
    all_ds = datasets.load_dataset(
        "json", data_files=str(UNIFIED_ALL_FILE), split="train"
    )
    print(f"  Loaded: {UNIFIED_ALL_FILE} ({len(all_ds)} rows)")

    # Group by source field
    source_names = sorted(set(all_ds["source"]))
    result = {}
    for source in source_names:
        subset = all_ds.filter(lambda x, s=source: x["source"] == s)
        result[source] = subset
        print(f"    {source}: {len(subset)} rows")

    return result


# ============================================================
# Main Entry
# ============================================================

def main():
    """Command line entry point, parse --stage argument and execute corresponding processing stage."""
    parser = argparse.ArgumentParser(
        description="Data Processing Pipeline: Parse raw data and generate formatted data for various training stages."
    )
    parser.add_argument(
        "--stage",
        type=str,
        required=True,
        choices=[
            "parse", "synthesize", "format_sft",
            "format_grpo", "format_reward", "all", "full",
        ],
        help="Processing stage to execute (all=exclude synthesis, full=include synthesis)",
    )
    parser.add_argument(
        "--n_headline", type=int, default=200,
        help="synthesize stage: Number of headlines to synthesize per language (default 200)",
    )
    parser.add_argument(
        "--n_keyword", type=int, default=100,
        help="synthesize stage: Number of keywords to synthesize per language (default 100)",
    )
    parser.add_argument(
        "--seed", type=int, default=42,
        help="Random seed (default 42)",
    )

    args = parser.parse_args()

    stage = args.stage
    run_synth = stage in ("synthesize", "full")
    run_all = stage in ("all", "full")

    if stage == "parse" or run_all:
        print("=" * 60)
        print("Stage: parse (Raw Data → Unified Intermediate Format)")
        print("=" * 60)
        run_parse()

    if run_synth:
        print("=" * 60)
        print("Stage: synthesize (Synthesize Type B Data, Requires Gemini API)")
        print("=" * 60)
        run_synthesize(
            n_headline=args.n_headline,
            n_keyword=args.n_keyword,
            seed=args.seed,
        )

    if stage == "format_sft" or run_all:
        print("=" * 60)
        print("Stage: format_sft (Unified Intermediate Format → SFT Data)")
        print("=" * 60)
        run_format_sft()

    if stage == "format_grpo" or run_all:
        print("=" * 60)
        print("Stage: format_grpo (SemEval → GRPO Prompt)")
        print("=" * 60)
        run_format_grpo()

    if stage == "format_reward" or run_all:
        print("=" * 60)
        print("Stage: format_reward (Unified Intermediate Format → Preference Pairs)")
        print("=" * 60)
        run_format_reward()

    print()
    print("Done.")


if __name__ == "__main__":
    main()

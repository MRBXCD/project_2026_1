"""
Type B Data Synthesis Script
============================

This script is independent of pipeline.py and is used to synthesize SFT Type B data (task formatted training data).

Synthesis Flow:
    1. Extract headlines from external news headline dataset (Babel Briefings) → headline subtask
    2. Randomly pair two uncommon words from predefined vocabulary → keyword subtask
    3. Call strong model API (Google Gemini) to generate humor responses satisfying constraints
    4. Quality filtering (keyword inclusion check, length check, non-empty check)
    5. Wrap as SFT messages format, save as JSONL

Important Design Principles:
    - Do not use headlines/keywords provided by SemEval (prevent data leakage)
    - Headlines come from external news dataset
    - Keyword pairs are randomly generated by script
    - Synthesis results saved to data/synthesized/type_b_{lang}.jsonl
    - format_sft stage in pipeline.py will automatically load these files

Usage:
    # Synthesize English data (Default 200 headlines + 100 keywords)
    python -m data_preprocessing.synthesize_task_data --lang en

    # Synthesize Chinese data, specify quantity
    python -m data_preprocessing.synthesize_task_data --lang zh --n_headline 300 --n_keyword 150

    # Synthesize all three languages
    python -m data_preprocessing.synthesize_task_data --lang all

Dependencies:
    - google-genai (Google Gemini API)
    - prompt_templates (Local)
    - datasets (HuggingFace, for loading Babel Briefings)

Environment Variables:
    - GEMINI_API_KEY: Google Gemini API Key (Must be set)
"""

import argparse
import itertools
import json
import os
import random
import time
from pathlib import Path

from data_preprocessing.prompt_templates import (
    build_headline_prompt,
    build_keyword_prompt,
)

# ============================================================
# Path Constants
# ============================================================

PROJECT_ROOT = Path(__file__).resolve().parent.parent
SYNTHESIZED_DIR = PROJECT_ROOT / "data" / "synthesized"


# ============================================================
# Keyword Vocabulary (For keyword subtask)
# ============================================================
# Principles for selecting these word pairs:
#   - Do not overlap with keywords provided by SemEval
#   - Cover various POS and semantic categories (animals, objects, actions, food, etc.)
#   - Create "incongruity" when paired, helping humor effect
#
# Maintain an independent vocabulary for each language.
# The synthesis flow for keyword subtask will randomly pick two words to pair from here.

KEYWORD_POOL_EN = [
    "astronaut", "cactus", "piano", "tornado", "kangaroo",
    "umbrella", "volcano", "noodle", "submarine", "giraffe",
    "avalanche", "toaster", "jellyfish", "telescope", "pretzel",
    "dinosaur", "trampoline", "mushroom", "lighthouse", "accordion",
    "parrot", "tuxedo", "glacier", "chopstick", "saxophone",
    "elevator", "flamingo", "suitcase", "comet", "waffle",
]

KEYWORD_POOL_ZH = [
    "宇航员", "仙人掌", "钢琴", "龙卷风", "袋鼠",
    "雨伞", "火山", "面条", "潜水艇", "长颈鹿",
    "雪崩", "烤面包机", "水母", "望远镜", "恐龙",
    "蹦床", "蘑菇", "灯塔", "手风琴", "鹦鹉",
    "冰川", "筷子", "萨克斯", "电梯", "火烈鸟",
    "行李箱", "彗星", "华夫饼", "拖拉机", "企鹅",
]

KEYWORD_POOL_ES = [
    "astronauta", "cactus", "piano", "tornado", "canguro",
    "paraguas", "volcán", "fideos", "submarino", "jirafa",
    "avalancha", "tostadora", "medusa", "telescopio", "pretzel",
    "dinosaurio", "trampolín", "hongo", "faro", "acordeón",
    "loro", "esmoquin", "glaciar", "palillos", "saxofón",
    "ascensor", "flamenco", "maleta", "cometa", "gofre",
]

_KEYWORD_POOLS = {
    "en": KEYWORD_POOL_EN,
    "zh": KEYWORD_POOL_ZH,
    "es": KEYWORD_POOL_ES,
}


# ============================================================
# Gemini API Call
# ============================================================

def _init_gemini_client():
    """Initialize Google Gemini API Client.

    Requires environment variable GEMINI_API_KEY.

    Returns:
        google.genai.Client instance

    Raises:
        ValueError: GEMINI_API_KEY environment variable not set
    """
    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        raise ValueError(
            "GEMINI_API_KEY environment variable not set.\n"
            "Please run: export GEMINI_API_KEY='your-api-key'"
        )

    from google import genai
    client = genai.Client(api_key=api_key)
    print("  Gemini API client initialized successfully")
    return client


def _call_gemini(client, prompt: str, max_retries: int = 3) -> str | None:
    """Call Gemini API to generate response, with retry and rate limit handling.

    Args:
        client: Gemini API client
        prompt: Prompt text sent to the model
        max_retries: Maximum number of retries (handling API rate limits or temporary errors)

    Returns:
        str: Text generated by the model, None if failed
    """
    from google.genai import types

    for attempt in range(1, max_retries + 1):
        try:
            response = client.models.generate_content(
                model="gemini-3-flash-preview",
                contents=prompt,
                config=types.GenerateContentConfig(
                    # Disable thinking mode, just need direct response
                    thinking_config=types.ThinkingConfig(thinking_budget=0),
                    # Higher temperature encourages creativity
                    temperature=0.9,
                    max_output_tokens=256,
                ),
            )
            text = response.text
            if text:
                return text.strip()
            return None

        except Exception as e:
            error_msg = str(e).lower()

            # API Rate Limit (429 Too Many Requests) or Service Temporarily Unavailable (503)
            if "429" in error_msg or "resource" in error_msg or "quota" in error_msg:
                wait_time = 2 ** attempt * 5  # Exponential backoff: 10s, 20s, 40s
                print(f"    API Rate Limit, waiting {wait_time}s before retry ({attempt}/{max_retries})")
                time.sleep(wait_time)
                continue

            # Other errors
            print(f"    Gemini call failed ({attempt}/{max_retries}): {e}")
            if attempt < max_retries:
                time.sleep(2)
                continue

    return None


# ============================================================
# News Headline Acquisition (Babel Briefings)
# ============================================================

def _load_headlines(lang: str, n_samples: int, seed: int = 42) -> list[str]:
    """Randomly sample specified number of news headlines from Babel Briefings dataset.

    Babel Briefings is a multilingual news headline dataset (HuggingFace: felixludos/babel-briefings),
    containing 4.7 million news headlines in 30 languages including English, Chinese, Spanish, etc.

    Args:
        lang: Language code "en" / "zh" / "es"
        n_samples: Number of headlines required
        seed: Random seed

    Returns:
        list[str]: List of news headlines
    """
    import datasets as ds_lib

    print(f"  Loading {lang} headlines from Babel Briefings (streaming mode)...")

    # Use streaming=True to avoid downloading full ~5GB dataset
    dataset = ds_lib.load_dataset(
        "felixludos/babel-briefings",
        split="train",
        streaming=True,
    )

    # Filter by language, collect enough candidate headlines
    # Collect more (3x) to have sufficient pool for random sampling later
    collect_target = n_samples * 3
    candidates = []

    for example in dataset:
        if example.get("language") == lang:
            title = example.get("title", "")
            # Basic filter: non-empty, reasonable length (not too short or too long)
            if title and 10 <= len(title) <= 300:
                candidates.append(title)
            if len(candidates) >= collect_target:
                break

    print(f"  Collected {len(candidates)} candidate headlines")

    if len(candidates) == 0:
        raise ValueError(f"Failed to fetch {lang} headlines from Babel Briefings")

    # Random sampling
    rng = random.Random(seed)
    rng.shuffle(candidates)
    selected = candidates[:n_samples]

    print(f"  Selected {len(selected)} headlines")
    return selected


# ============================================================
# Keyword Pair Generation
# ============================================================

def _generate_keyword_pairs(lang: str, n_pairs: int, seed: int = 42) -> list[tuple[str, str]]:
    """Randomly pair keywords from pool to generate unique keyword pairs.

    Pairing rules:
    - Two words are not identical
    - Same combination not repeated (Order irrelevant: (a,b) and (b,a) are same)

    Args:
        lang: Language code
        n_pairs: Number of pairs to generate
        seed: Random seed

    Returns:
        list[tuple[str, str]]: List of keyword pairs
    """
    if lang not in _KEYWORD_POOLS:
        raise ValueError(f"Unsupported language code: '{lang}'")

    pool = _KEYWORD_POOLS[lang]

    # Generate all unique combinations of two
    all_combos = list(itertools.combinations(pool, 2))

    if n_pairs > len(all_combos):
        print(
            f"  Warning: Requested {n_pairs} keyword pairs, but vocabulary can only generate {len(all_combos)} combinations, "
            f"using all combinations"
        )
        n_pairs = len(all_combos)

    rng = random.Random(seed)
    selected = rng.sample(all_combos, n_pairs)

    print(f"  Generated {len(selected)} keyword pairs")
    return selected


# ============================================================
# Quality Filtering
# ============================================================

def _filter_headline_response(response: str) -> bool:
    """Check if the response generated for headline subtask is qualified.

    Filter conditions:
    - Non-empty
    - Reasonable length (10 ~ 500 characters)
    - Does not contain common refusal patterns ("I cannot", "I'm sorry", etc.)

    Args:
        response: Response text generated by model

    Returns:
        bool: True if qualified
    """
    if not response or not response.strip():
        return False

    text = response.strip()

    # Length check
    if len(text) < 10 or len(text) > 500:
        return False

    # Refusal pattern check (Model sometimes refuses to generate humor)
    refusal_patterns = [
        "i cannot", "i can't", "i'm sorry", "i apologize",
        "as an ai", "as a language model",
        "不能", "抱歉", "对不起", "作为一个ai", "作为语言模型",
        "no puedo", "lo siento", "disculpa",
    ]
    text_lower = text.lower()
    for pattern in refusal_patterns:
        if pattern in text_lower:
            return False

    return True


def _filter_keyword_response(response: str, word1: str, word2: str) -> bool:
    """Check if the response generated for keyword subtask is qualified.

    Filter conditions:
    - Meets all conditions of _filter_headline_response
    - Extra: Response must contain both word1 and word2 (case-insensitive)

    Args:
        response: Response text generated by model
        word1: First required keyword
        word2: Second required keyword

    Returns:
        bool: True if qualified
    """
    # Check general conditions first
    if not _filter_headline_response(response):
        return False

    # Extra check: both keywords must appear in response (case-insensitive)
    text_lower = response.lower()
    if word1.lower() not in text_lower:
        return False
    if word2.lower() not in text_lower:
        return False

    return True


# ============================================================
# Synthesis Main Logic
# ============================================================

def synthesize_for_language(
    lang: str,
    n_headline: int = 200,
    n_keyword: int = 100,
    seed: int = 42,
) -> list[dict]:
    """Synthesize Type B data for specified language.

    Performed in two parts:
    1. Headline Subtask: Sample headlines from Babel Briefings → Gemini generates jokes
    2. Keyword Subtask: Randomly pair keywords → Gemini generates jokes containing keywords

    Each synthesized sample is directly wrapped into SFT messages format.

    Args:
        lang: Language code "en" / "zh" / "es"
        n_headline: Number of headline subtask samples to synthesize
        n_keyword: Number of keyword subtask samples to synthesize
        seed: Random seed

    Returns:
        list[dict]: Synthesized SFT sample list, each element format:
            {"messages": [{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]}
    """
    # Oversample ratio: Prepare 1.5x input materials to compensate for quality filtering loss.
    # Stop early once target quantity is reached during generation, to save API calls.
    OVERSAMPLE_RATIO = 1.5

    client = _init_gemini_client()
    all_samples = []

    # ---- Headline Part ----
    if n_headline > 0:
        print(f"\n  --- Headline Subtask (Target: {n_headline}) ---")

        # Prepare 1.5x headlines for filtering margin
        n_headline_fetch = int(n_headline * OVERSAMPLE_RATIO)
        headlines = _load_headlines(lang, n_headline_fetch, seed=seed)

        passed = 0
        failed = 0
        for i, headline in enumerate(headlines):
            # Target reached, stop early
            if passed >= n_headline:
                print(f"    Target {n_headline} reached, stopping early (Total API calls: {i})")
                break

            user_prompt = build_headline_prompt(headline, lang)
            response = _call_gemini(client, user_prompt)

            if response and _filter_headline_response(response):
                all_samples.append({
                    "messages": [
                        {"role": "user", "content": user_prompt},
                        {"role": "assistant", "content": response},
                    ]
                })
                passed += 1
            else:
                failed += 1

            # Progress print (every 50)
            if (i + 1) % 50 == 0:
                print(f"    Progress: API calls {i + 1}/{len(headlines)}, Passed: {passed}/{n_headline}, Filtered: {failed}")

            time.sleep(0.1)

        print(f"  Headline Done: {passed} passed, {failed} filtered")
        if passed < n_headline:
            print(f"  Warning: Target not reached (Target {n_headline}, Actual {passed}), try increasing OVERSAMPLE_RATIO")

    # ---- Keyword Part ----
    if n_keyword > 0:
        print(f"\n  --- Keyword Subtask (Target: {n_keyword}) ---")

        # Prepare 1.5x keyword pairs
        n_keyword_fetch = int(n_keyword * OVERSAMPLE_RATIO)
        keyword_pairs = _generate_keyword_pairs(lang, n_keyword_fetch, seed=seed)

        passed = 0
        failed = 0
        for i, (w1, w2) in enumerate(keyword_pairs):
            if passed >= n_keyword:
                print(f"    Target {n_keyword} reached, stopping early (Total API calls: {i})")
                break

            user_prompt = build_keyword_prompt(w1, w2, lang)
            response = _call_gemini(client, user_prompt)

            if response and _filter_keyword_response(response, w1, w2):
                all_samples.append({
                    "messages": [
                        {"role": "user", "content": user_prompt},
                        {"role": "assistant", "content": response},
                    ]
                })
                passed += 1
            else:
                failed += 1

            if (i + 1) % 50 == 0:
                print(f"    Progress: API calls {i + 1}/{len(keyword_pairs)}, Passed: {passed}/{n_keyword}, Filtered: {failed}")

            time.sleep(0.1)

        print(f"  Keyword Done: {passed} passed, {failed} filtered")
        if passed < n_keyword:
            print(f"  Warning: Target not reached (Target {n_keyword}, Actual {passed}), try increasing OVERSAMPLE_RATIO")

    print(f"\n  Total synthesized: {len(all_samples)} samples")
    return all_samples


def save_synthesized(samples: list[dict], lang: str) -> Path:
    """Save synthesized samples as JSONL file.

    Args:
        samples: Synthesized SFT sample list
        lang: Language code

    Returns:
        Path: Saved file path
    """
    SYNTHESIZED_DIR.mkdir(parents=True, exist_ok=True)

    output_path = SYNTHESIZED_DIR / f"type_b_{lang}.jsonl"

    with open(output_path, "w", encoding="utf-8") as f:
        for sample in samples:
            f.write(json.dumps(sample, ensure_ascii=False) + "\n")

    return output_path


# ============================================================
# Command Line Entry
# ============================================================

def main():
    """Command line entry point, parse arguments and run synthesis."""
    parser = argparse.ArgumentParser(
        description="Synthesize Type B Task Formatted Data (Requires Gemini API)"
    )
    parser.add_argument(
        "--lang",
        type=str,
        required=True,
        choices=["en", "zh", "es", "all"],
        help="Target language, or 'all' to synthesize all three languages",
    )
    parser.add_argument(
        "--n_headline",
        type=int,
        default=200,
        help="Number of headline subtask samples per language (default 200)",
    )
    parser.add_argument(
        "--n_keyword",
        type=int,
        default=100,
        help="Number of keyword subtask samples per language (default 100)",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Random seed (default 42)",
    )

    args = parser.parse_args()

    languages = ["en", "zh", "es"] if args.lang == "all" else [args.lang]

    for lang in languages:
        print(f"\n{'=' * 60}")
        print(f"Synthesizing Type B data: lang={lang}")
        print(f"  headline: {args.n_headline} rows, keyword: {args.n_keyword} rows")
        print(f"{'=' * 60}")

        samples = synthesize_for_language(
            lang=lang,
            n_headline=args.n_headline,
            n_keyword=args.n_keyword,
            seed=args.seed,
        )

        output_path = save_synthesized(samples, lang)
        print(f"\nSaved: {output_path} ({len(samples)} rows)")

    print("\nDone.")


if __name__ == "__main__":
    main()

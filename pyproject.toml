[project]
name = "project-2026-1"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.11"
dependencies = [
    "accelerate>=1.0.0",
    "bitsandbytes>=0.49.2",
    "datasets>=4.6.0",
    "einops>=0.8.2",
    "emoji>=2.15.0",
    "evaluate>=0.4.6",
    "flash-attn",
    "google-genai>=1.65.0",
    "ipywidgets>=8.1.8",
    "jupyterlab>=4.5.5",
    "lm-eval>=0.4.11",
    "matplotlib>=3.10.8",
    "numpy>=2.4.2",
    "pandas>=3.0.1",
    "peft>=0.18.1",
    "python-dotenv>=1.2.1",
    "rouge-score>=0.1.2",
    "sacrebleu>=2.6.0",
    "safetensors>=0.7.0",
    "scikit-learn>=1.8.0",
    "seaborn>=0.13.2",
    "sentencepiece>=0.2.1",
    "tiktoken>=0.12.0",
    "tokenizers>=0.22.2",
    "tqdm>=4.67.3",
    "transformers>=4.45.0",
    "trl>=0.29.0",
    "wandb>=0.25.0",
]

[tool.uv.sources]
flash-attn = { url = "https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.8cxx11abiTRUE-cp311-cp311-linux_x86_64.whl" }

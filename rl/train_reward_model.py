"""
Reward Model Training Script
=============================

Train a lightweight reward model (e.g., Qwen3-1.7B) on preference pair data
to provide a stable, fast, and task-specific humor quality signal for GRPO
training. This replaces the Gemini LLM-as-Judge approach which suffers from
high noise, high latency, and coarse scoring granularity.

Training Overview:
    The reward model is a sequence classification model (base LM + linear
    score head) trained with the Bradley-Terry preference loss. Given a
    (prompt, chosen, rejected) triple, the model learns to assign a higher
    scalar reward to the chosen response than the rejected one.

    TRL's RewardTrainer handles:
    - Loading the base model and adding a classification head.
    - Converting conversational data to tokenized inputs.
    - Computing the Bradley-Terry loss: -log(sigma(r_chosen - r_rejected)).
    - LoRA adapter training with PEFT.

LoRA Strategy:
    The base LM layers are trained with LoRA (r=16), while the newly added
    ``score`` head (a linear layer projecting hidden states to a scalar)
    must be fully trainable. This is achieved via modules_to_save=["score"]
    in the LoRA config.

Data:
    Uses preference pairs from data/reward/preference_{train,val}.jsonl,
    generated by data_preprocessing.formatters.format_reward_pairs().
    Format: {prompt: [...], chosen: [...], rejected: [...]} in TRL
    conversational format.

Usage:
    python -m rl.train_reward_model
    python -m rl.train_reward_model --model_name Qwen/Qwen3-1.7B --lr 1e-5
    python -m rl.train_reward_model --lora_rank 16 --batch_size 8

Dependencies:
    - transformers, trl (>= 0.27), peft, torch, datasets, accelerate
    - wandb (optional, for experiment tracking)
"""

import argparse
from datetime import datetime
from pathlib import Path

import torch
from datasets import load_dataset
from peft import LoraConfig, TaskType
from trl import RewardConfig, RewardTrainer


# ============================================================
# Path Constants
# ============================================================

PROJECT_ROOT = Path(__file__).resolve().parent.parent
PREFERENCE_TRAIN_FILE = PROJECT_ROOT / "data" / "reward" / "preference_train.jsonl"
PREFERENCE_VAL_FILE = PROJECT_ROOT / "data" / "reward" / "preference_val.jsonl"
REWARD_MODEL_CHECKPOINT_DIR = PROJECT_ROOT / "checkpoints" / "reward_model"


# ============================================================
# Dataset Loading
# ============================================================

def load_preference_dataset(
    train_file: str | Path = PREFERENCE_TRAIN_FILE,
    val_file: str | Path = PREFERENCE_VAL_FILE,
) -> tuple:
    """Load preference pair datasets for reward model training.

    Expected JSONL format (one JSON object per line):
        {
            "prompt":   [{"role": "user", "content": "Tell me a joke."}],
            "chosen":   [{"role": "assistant", "content": "Funny joke..."}],
            "rejected": [{"role": "assistant", "content": "Weak joke..."}]
        }

    This matches TRL RewardTrainer's conversational format with explicit
    prompt. The trainer automatically applies the chat template during
    tokenization.

    Args:
        train_file: Path to training preference pairs JSONL.
        val_file: Path to validation preference pairs JSONL.

    Returns:
        tuple: (train_dataset, val_dataset)

    Raises:
        FileNotFoundError: If either data file does not exist.
    """
    train_file = Path(train_file)
    val_file = Path(val_file)

    if not train_file.exists():
        raise FileNotFoundError(
            f"Preference training data not found at {train_file}. "
            f"Please run: python -m data_preprocessing.pipeline --stage format_reward"
        )
    if not val_file.exists():
        raise FileNotFoundError(
            f"Preference validation data not found at {val_file}. "
            f"Please run: python -m data_preprocessing.pipeline --stage format_reward"
        )

    train_ds = load_dataset("json", data_files=str(train_file), split="train")
    val_ds = load_dataset("json", data_files=str(val_file), split="train")

    print(f"Loaded preference data:")
    print(f"  Train: {len(train_ds)} pairs from {train_file.name}")
    print(f"  Val:   {len(val_ds)} pairs from {val_file.name}")
    print(f"  Columns: {train_ds.column_names}")

    return train_ds, val_ds


# ============================================================
# LoRA Configuration for Reward Model
# ============================================================

def build_reward_lora_config(
    rank: int = 16,
    alpha: int = 32,
) -> LoraConfig:
    """Build LoRA configuration for the reward model.

    Uses a smaller rank (16) than SFT (64) or GRPO (32) because the
    reward model only needs to learn a scoring function, not generate
    text. The ``score`` layer (added by AutoModelForSequenceClassification)
    is included in modules_to_save so it is fully trainable rather than
    wrapped by LoRA.

    Args:
        rank: LoRA rank. Default 16.
        alpha: LoRA scaling factor. Usually 2 * rank. Default 32.

    Returns:
        LoraConfig: PEFT configuration for reward model training.
    """
    config = LoraConfig(
        task_type=TaskType.SEQ_CLS,
        r=rank,
        lora_alpha=alpha,
        lora_dropout=0.05,
        target_modules=[
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj",
        ],
        modules_to_save=["score"],
    )

    print(f"  Reward LoRA: r={rank}, alpha={alpha}, dropout={config.lora_dropout}")
    print(f"  modules_to_save: {config.modules_to_save}")
    return config


# ============================================================
# Reward Training Configuration
# ============================================================

def build_reward_config(
    batch_size: int = 8,
    grad_accum: int = 2,
    learning_rate: float = 1e-5,
    num_epochs: int = 1,
    max_length: int = 512,
    report_to: str = "wandb",
) -> RewardConfig:
    """Build the RewardConfig for training.

    Key hyperparameter considerations:

    Epochs:
        1 epoch is usually sufficient for reward models trained on large
        preference datasets (151K pairs). More epochs risk overfitting to
        specific joke patterns rather than learning general humor quality.

    Learning rate:
        1e-5 is standard for reward model LoRA training. Larger than GRPO
        (5e-6) because the reward model has a fresh score head that needs
        meaningful gradient updates from the start.

    Max length:
        512 tokens is sufficient for jokes (typically <100 tokens). The
        preference data includes prompt + response, still well within 512.

    Args:
        batch_size: Per-device batch size. Default 8.
        grad_accum: Gradient accumulation steps. Default 2.
        learning_rate: Learning rate. Default 1e-5.
        num_epochs: Training epochs. Default 1.
        max_length: Maximum sequence length. Default 512.
        report_to: Experiment tracking backend. Default "wandb".

    Returns:
        RewardConfig: Training configuration for RewardTrainer.
    """
    effective_batch = batch_size * grad_accum
    print(f"  Effective batch size: {batch_size} x {grad_accum} = {effective_batch}")

    return RewardConfig(
        output_dir=str(REWARD_MODEL_CHECKPOINT_DIR),

        # --- Training ---
        num_train_epochs=num_epochs,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        gradient_accumulation_steps=grad_accum,
        learning_rate=learning_rate,
        lr_scheduler_type="cosine",
        warmup_ratio=0.05,

        # --- Sequence length ---
        max_length=max_length,

        # --- Precision ---
        bf16=True,

        # --- Memory ---
        gradient_checkpointing=True,

        # --- Logging and Saving ---
        logging_steps=50,
        eval_strategy="steps",
        eval_steps=500,
        save_strategy="steps",
        save_steps=500,
        save_total_limit=3,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        report_to=report_to,
        seed=42,
        run_name=f"reward_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
    )


# ============================================================
# Main Training Flow
# ============================================================

def main():
    """Reward model training main entry point.

    Pipeline:
    1. Parse command-line arguments.
    2. Load preference pair dataset.
    3. Build LoRA config (with modules_to_save=["score"]).
    4. Build RewardConfig (training hyperparameters).
    5. Create RewardTrainer (handles model loading + classification head).
    6. Train.
    7. Save final checkpoint.
    """
    parser = argparse.ArgumentParser(description="Reward Model Training Script")
    parser.add_argument(
        "--model_name", type=str, default="Qwen/Qwen3-1.7B",
        help="Base model name or path (default: Qwen/Qwen3-1.7B)",
    )
    parser.add_argument(
        "--batch_size", type=int, default=8,
        help="Per-device batch size (default: 8)",
    )
    parser.add_argument(
        "--grad_accum", type=int, default=2,
        help="Gradient accumulation steps (default: 2)",
    )
    parser.add_argument(
        "--lr", type=float, default=1e-5,
        help="Learning rate (default: 1e-5)",
    )
    parser.add_argument(
        "--num_epochs", type=int, default=1,
        help="Number of training epochs (default: 1)",
    )
    parser.add_argument(
        "--lora_rank", type=int, default=16,
        help="LoRA rank for reward model (default: 16)",
    )
    parser.add_argument(
        "--max_length", type=int, default=512,
        help="Maximum sequence length (default: 512)",
    )
    parser.add_argument(
        "--report_to", type=str, default="wandb",
        choices=["wandb", "tensorboard", "none"],
        help="Experiment tracking backend (default: wandb)",
    )
    args = parser.parse_args()

    # ---- Step 1: Load Dataset ----
    print("=" * 60)
    print("Step 1: Load preference pair dataset")
    print("=" * 60)
    train_ds, val_ds = load_preference_dataset()

    # ---- Step 2: Build LoRA Config ----
    print("\n" + "=" * 60)
    print("Step 2: Configure reward model LoRA")
    print("=" * 60)
    lora_config = build_reward_lora_config(
        rank=args.lora_rank,
        alpha=args.lora_rank * 2,
    )

    # ---- Step 3: Build RewardConfig ----
    print("\n" + "=" * 60)
    print("Step 3: Configure training hyperparameters")
    print("=" * 60)
    reward_config = build_reward_config(
        batch_size=args.batch_size,
        grad_accum=args.grad_accum,
        learning_rate=args.lr,
        num_epochs=args.num_epochs,
        max_length=args.max_length,
        report_to=args.report_to,
    )

    # ---- Step 4: Create RewardTrainer ----
    print("\n" + "=" * 60)
    print(f"Step 4: Create RewardTrainer (base model: {args.model_name})")
    print("=" * 60)
    trainer = RewardTrainer(
        model=args.model_name,
        args=reward_config,
        train_dataset=train_ds,
        eval_dataset=val_ds,
        peft_config=lora_config,
    )
    print("  RewardTrainer created successfully.")
    trainable = sum(p.numel() for p in trainer.model.parameters() if p.requires_grad)
    total = sum(p.numel() for p in trainer.model.parameters())
    print(f"  Trainable parameters: {trainable / 1e6:.1f}M / {total / 1e6:.1f}M "
          f"({trainable / total * 100:.2f}%)")

    # ---- Step 5: Train ----
    print("\n" + "=" * 60)
    print("Step 5: Start reward model training")
    print("=" * 60)
    trainer.train()

    # ---- Step 6: Save Final Model ----
    print("\n" + "=" * 60)
    print("Step 6: Save final model")
    print("=" * 60)
    final_dir = REWARD_MODEL_CHECKPOINT_DIR / "final"
    trainer.save_model(str(final_dir))
    trainer.processing_class.save_pretrained(str(final_dir))
    print(f"  Model saved to: {final_dir}")

    print("\nReward model training complete.")


if __name__ == "__main__":
    main()
